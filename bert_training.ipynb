{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef7161a-d8ab-4564-bc35-021972b26d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT-alapú osztályozó betöltése\n",
    "# Modell: \"google/bert_uncased_L-2_H-128_A-2\" (2 osztály)\n",
    "# Tokenizer és előtanított modell inicializálása\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "num_labels = 2\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a765cd6e-d6f3-44ec-a403-d0329c00c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6215184332ec4e73b0f75f4bf0173b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 70566\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3921\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3920\n",
      "    })\n",
      "})\n",
      "{'text': 'WELLINGTON (Reuters) - Just days after the United States said it would increase troop numbers in Afghanistan and ask its allies to do the same, New Zealand on Friday announced an extra three non-combat military personnel, boosting its military commitment to 13. U.S. President Donald Trump on Monday unveiled his strategy to end the conflict in Afghanistan, committing the United States to an open-ended conflict and signaling he would dispatch more troops to America s longest war. U.S. officials have said Trump had signed off on plans to send about 4,000 more U.S. troops to add to the roughly 8,400 now deployed in Afghanistan. U.S. Defense Secretary James Mattis has since said exact troop numbers are yet to be decided. Trump said he would ask coalition allies to support his new strategy, with additional troops and funding, to end the 16-year conflict. New Zealand Defence Minister Mark Mitchell s announcement boosting the country s Kabul-based troops to 13 follows a request for NATO (National Atlantic Treaty Organization) to send more troops to Afghanistan earlier this year. New Zealand has had troops in Afghanistan since 2001. Its presence has been decreasing since 2013 but it has kept some personnel on the ground to train local officers.  New Zealand will continue to stand alongside our partners in supporting stability in Afghanistan and countering the threat of international terrorism,  said Mitchell. Prime Minister Bill English said the government has ruled out making a decision on sending combat troops to Afghanistan before New Zealand s election on Sept. 23. Opposition leader Jacinda Ardern told local media this week she would not back sending troops to Afghanistan at the moment but was not privy to intelligence such decisions were based on. ', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Előkészített adathalmaz betöltése és előfeldolgozása\n",
    "# - CSV beolvasása és hiányzó/üres szövegek kiszűrése\n",
    "# - 90%-os tanító és 10%-os teszt felosztás\n",
    "# - Teszt tovább bontása 50-50%-ban validációs és végső tesztre\n",
    "# - Végső struktúra: train, validation, test\n",
    "# - Teszt készlet mentése CSV-be és szerkezet kiírása\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"all_data\": \"resource_data/cleaned_fake_news_dataset.csv\"})\n",
    "\n",
    "# Filter out rows with missing or empty text\n",
    "dataset[\"all_data\"] = dataset[\"all_data\"].filter(lambda example: example[\"text\"] is not None and example[\"text\"].strip() != \"\")\n",
    "\n",
    "# Step 1: Split into 90% train, 10% test\n",
    "split_data = dataset[\"all_data\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Step 2: Split test set into validation (50%) and final test (50%)\n",
    "test_valid_split = split_data[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Create final dataset dictionary\n",
    "split_data = DatasetDict({\n",
    "    \"train\": split_data[\"train\"],\n",
    "    \"test\": test_valid_split[\"test\"],\n",
    "    \"validation\": test_valid_split[\"train\"],\n",
    "})\n",
    "\n",
    "# Save test set to CSV\n",
    "split_data[\"test\"].to_csv(\"resource_data/fake_news_dataset_test.csv\")\n",
    "\n",
    "# Print structure and first item\n",
    "print(split_data)\n",
    "print(split_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fbd06e-bb37-496f-8c0b-6f907601e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ellenőrzés hibás sorokra\n",
    "for i, example in enumerate(split_data[\"train\"]):\n",
    "    if not isinstance(example[\"text\"], str):\n",
    "        print(f\"Non-string type found at index {i}: {example['text']} (type: {type(example['text'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4097a052-af33-447b-b18b-2a79e64feefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e1f899c904408c9649f7aae4ada113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b02a868e5f47619ef5abf790fa94b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8306cc540a45b3b2c42ccb79d528db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 70566\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3921\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3920\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenizálás BERT modellhez\n",
    "# - Szövegek max 128 hosszra vágása és párnázása\n",
    "# - Minden adathalmazon végrehajtás\n",
    "# - Tokenizált adatok ellenőrzése\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization to all dataset splits\n",
    "tokenized_datasets = split_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check if tokenization worked\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de8f3e2-19d0-4d6c-a082-32b80de1e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanítási paraméterek beállítása\n",
    "# - Kimeneti mappa: \"results\"\n",
    "# - Értékelés és mentés epochonként\n",
    "# - Tanulási ráta: 2e-5, súlycsökkenés: 0.01\n",
    "# - Batch méret: 16 (train és eval)\n",
    "# - 3 tanítási epoch, legjobb modell betöltése a végén\n",
    "# - Float16 (fp16) gyorsítás engedélyezve\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e805e5-584d-4ed8-9975-eee82cf89e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vorosg\\AppData\\Local\\Temp\\ipykernel_27604\\1661876755.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13233' max='13233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13233/13233 39:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.316821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>0.290768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.292643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13233, training_loss=0.3258444746362373, metrics={'train_runtime': 2346.0674, 'train_samples_per_second': 90.235, 'train_steps_per_second': 5.641, 'total_flos': 67239891348480.0, 'train_loss': 0.3258444746362373, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer inicializálása és tanítás indítása\n",
    "# - Adatkitöltés tokenizálás után\n",
    "# - Modell, tanítási paraméterek és adathalmazok beállítása\n",
    "# - Tanítás elindítása a megadott konfigurációval\n",
    "\n",
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4556ffe-27c6-4b05-b75d-da2769df23a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fake_news_bert_model\\\\tokenizer_config.json',\n",
       " 'fake_news_bert_model\\\\special_tokens_map.json',\n",
       " 'fake_news_bert_model\\\\vocab.txt',\n",
       " 'fake_news_bert_model\\\\added_tokens.json',\n",
       " 'fake_news_bert_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fake_news_bert_model\")\n",
    "tokenizer.save_pretrained(\"fake_news_bert_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
